{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMU BIA Datathon EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pre-processing\n",
    "import re\n",
    "import nltk\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from span_marker import SpanMarkerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### news_excerpts_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Excel file\n",
    "news_excerpts_parsed_path = r'/Users/ShanShan/SMU-Datathon-2025/datasets/input/news_excerpts_parsed.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "news_excerpts_parsed_df = pd.read_excel(news_excerpts_parsed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1509 entries, 0 to 1508\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Link    1509 non-null   object\n",
      " 1   Text    1509 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 23.7+ KB\n"
     ]
    }
   ],
   "source": [
    "news_excerpts_parsed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1509, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_excerpts_parsed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://edition.cnn.com/2023/09/29/business/st...</td>\n",
       "      <td>Starbucks violated federal labor law when it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/su-w...</td>\n",
       "      <td>The first suspect to plead guilty in Singapore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://edition.cnn.com/2023/05/22/tech/meta-f...</td>\n",
       "      <td>Meta has been fined a record-breaking €1.2 bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/bill...</td>\n",
       "      <td>SINGAPORE: A 45-year-old man linked to Singapo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://edition.cnn.com/2024/03/05/politics/li...</td>\n",
       "      <td>The Department of Education imposed a record $...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://edition.cnn.com/2023/09/29/business/st...   \n",
       "1  https://www.channelnewsasia.com/singapore/su-w...   \n",
       "2  https://edition.cnn.com/2023/05/22/tech/meta-f...   \n",
       "3  https://www.channelnewsasia.com/singapore/bill...   \n",
       "4  https://edition.cnn.com/2024/03/05/politics/li...   \n",
       "\n",
       "                                                Text  \n",
       "0  Starbucks violated federal labor law when it i...  \n",
       "1  The first suspect to plead guilty in Singapore...  \n",
       "2  Meta has been fined a record-breaking €1.2 bil...  \n",
       "3  SINGAPORE: A 45-year-old man linked to Singapo...  \n",
       "4  The Department of Education imposed a record $...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "news_excerpts_parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikileaks_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV file\n",
    "wikileaks_parsed_path = r'/Users/ShanShan/SMU-Datathon-2025/datasets/input/wikileaks_parsed.xlsx'\n",
    "\n",
    "# Read the CSV file\n",
    "wikileaks_parsed_df = pd.read_excel(wikileaks_parsed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   PDF Path  143 non-null    object\n",
      " 1   Text      143 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "wikileaks_parsed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikileaks_parsed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF Path</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.pdf</td>\n",
       "      <td>Pristina Airport – Possible administrative irr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.pdf</td>\n",
       "      <td>Investigative details\\n\\nIn his/her interviews...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.pdf</td>\n",
       "      <td>\"An interoffice memorandum providing an “outst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.pdf</td>\n",
       "      <td>\"Allegation 2 &amp; 3:\\n\\n(Specifically, three of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.pdf</td>\n",
       "      <td>\"When asked about this in interview, the Divis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PDF Path                                               Text\n",
       "0    1.pdf  Pristina Airport – Possible administrative irr...\n",
       "1    1.pdf  Investigative details\\n\\nIn his/her interviews...\n",
       "2   10.pdf  \"An interoffice memorandum providing an “outst...\n",
       "3   10.pdf  \"Allegation 2 & 3:\\n\\n(Specifically, three of ...\n",
       "4   10.pdf  \"When asked about this in interview, the Divis..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "wikileaks_parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### news_excerpts_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Link    0\n",
       "Text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "news_excerpts_parsed_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikileaks_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDF Path    0\n",
       "Text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "wikileaks_parsed_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### news_excerpts_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "news_excerpts_parsed_df_duplicates = news_excerpts_parsed_df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates in dataset:\", news_excerpts_parsed_df_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikileaks_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "wikileaks_parsed_df_duplicates = wikileaks_parsed_df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates in dataset:\", wikileaks_parsed_df_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### news_excerpts_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1508</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>https://cnalifestyle.channelnewsasia.com/dinin...</td>\n",
       "      <td>Starbucks violated federal labor law when it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Link  \\\n",
       "count                                                1509   \n",
       "unique                                               1508   \n",
       "top     https://cnalifestyle.channelnewsasia.com/dinin...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                     Text  \n",
       "count                                                1509  \n",
       "unique                                               1509  \n",
       "top     Starbucks violated federal labor law when it i...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_excerpts_parsed_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikileaks_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF Path</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>44</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2.pdf</td>\n",
       "      <td>Pristina Airport – Possible administrative irr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PDF Path                                               Text\n",
       "count       143                                                143\n",
       "unique       44                                                143\n",
       "top       2.pdf  Pristina Airport – Possible administrative irr...\n",
       "freq         17                                                  1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikileaks_parsed_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess_text:\n",
    "\n",
    "1. Converting to Lowercasing\n",
    "2. Removing Emojis\n",
    "3. Normalize Contractions\n",
    "4. Removing Punctuations\n",
    "5. Removal of Links\n",
    "6. Removing Special Characters\n",
    "7. Removing Mentions\n",
    "8. Removing Line breakers\n",
    "9. Removal of UTF-encoding\n",
    "10. Removing Hashtags\n",
    "11. Removing Special Characters\n",
    "12. Removing Extra Whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatize_and_remove_stopwords:\n",
    "1. Removing Stopwords\n",
    "2. Tokenization\n",
    "3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from the text.\"\"\"\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "def clean_hashtags(review):\n",
    "    \"\"\"Remove hashtags from the text.\"\"\"\n",
    "    # Remove hashtags at the end of the sentence\n",
    "    new_review = \" \".join(word.strip() for word in re.split(r'#(?!(?:hashtag)\\b)[\\w\\'-]+(?=(?:\\s+#[\\w\\'-]+)*\\s*$)', review))\n",
    "    # Remove '#' symbol from words in the middle of the sentence\n",
    "    new_review2 = \" \".join(word.strip() for word in re.split(r'#|_', new_review))\n",
    "    return new_review2\n",
    "\n",
    "def filter_chars(a):\n",
    "    \"\"\"Filter out unwanted characters from the text.\"\"\"\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text):\n",
    "    \"\"\"Remove multiple spaces in the text.\"\"\"\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "def normalize_contractions(text):\n",
    "    \"\"\"Normalize common English contractions.\"\"\"\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"i'm\": \"I am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"i've\": \"I have\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"would've\": \"would have\",  \n",
    "        \"i'd\": \"I would\",           \n",
    "        \"i'll\": \"I will\",           \n",
    "        \"there's\": \"there is\"       \n",
    "    }\n",
    "    # Replace contractions in the text\n",
    "    for contraction, full in contractions.items():\n",
    "        text = text.replace(contraction, full)\n",
    "    return text\n",
    "\n",
    "def normalize_punctuation(text):\n",
    "    \"\"\"Replace multiple consecutive periods and punctuation marks with a single instance and ensure spaces.\"\"\"\n",
    "    text = re.sub(r'\\.{2,}', '.', text)  # Normalize ellipses\n",
    "    text = re.sub(r'([!?.])\\1+', r'\\1', text)  # Normalize multiple punctuation marks\n",
    "    # Add space after punctuation if not at the end of the string\n",
    "    text = re.sub(r'([!?.])(?=\\S)', r'\\1 ', text)  \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool = True) -> str:\n",
    "    \"\"\"Preprocess the input text.\"\"\"\n",
    "\n",
    "# Check if the input is an integer, float, or contains only numeric characters\n",
    "    if isinstance(text, (int, float)) or (isinstance(text, str) and text.replace('.', '', 1).isdigit()):\n",
    "        return \"\"  # Return an empty string or any placeholder you'd like\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove emojis \n",
    "    text = remove_emojis(text)\n",
    "    # Normalize contractions\n",
    "    text = normalize_contractions(text)\n",
    "    # Normalize punctuation\n",
    "    text = normalize_punctuation(text)\n",
    "    text = re.sub(r'([!.,?;:])', r' \\1 ', text)  # Add spaces around punctuation before removal\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize extra spaces\n",
    "    # Remove double quotes\n",
    "    text = re.sub(r'\"', '', text)\n",
    "    # Normalize dashes\n",
    "    text = re.sub(r'-{2,}', '-', text)  # Replace multiple dashes with a single dash\n",
    "    text = re.sub(r'\\s*-\\s*', ' ', text)  # Remove isolated dashes\n",
    "    # Remove links\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "    # Remove specific special characters\n",
    "    text = re.sub(r'[\\\\/×\\^\\]:,.\\[÷]', '', text) \n",
    "    # Remove all spaces and line breaks\n",
    "    text = text.replace('\\r', '').replace('\\n', '')\n",
    "    # Remove UTF encodings\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text) \n",
    "    # Clean hashtags\n",
    "    text = clean_hashtags(text)\n",
    "    # Filter special characters\n",
    "    text = filter_chars(text)\n",
    "    # Remove multiple spaces\n",
    "    text = remove_mult_spaces(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lemmatize_and_remove_stopwords(text, remove_stopwords: bool = True):\n",
    "    \"\"\"Lemmatize the text and remove stopwords.\"\"\"\n",
    "    special_characters = r'[@_!#$%^&*()<>?/\\|}{~:]'\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [w for w in tokens if (not all(c in special_characters for c in w)) and (not w.isdigit())]\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "        tokens = [w.lower().strip() for w in tokens if w.lower() not in STOPWORDS]\n",
    "        return ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'text_' column in fake_reviews_df\n",
    "news_excerpts_parsed_df['cleaned_text'] = news_excerpts_parsed_df['Text'].apply(preprocess_text)\n",
    "\n",
    "news_excerpts_parsed_df['final_cleaned_text'] = news_excerpts_parsed_df['cleaned_text'].apply(lemmatize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'text_' column in fake_reviews_df\n",
    "wikileaks_parsed_df['cleaned_text'] = wikileaks_parsed_df['Text'].apply(preprocess_text)\n",
    "\n",
    "wikileaks_parsed_df['final_cleaned_text'] = wikileaks_parsed_df['cleaned_text'].apply(lemmatize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-mbert-base-multinerd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    entities = model.predict(text)  # Extract entities from the text\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Apply to datasets\n",
    "news_excerpts_parsed_df[\"entities\"] = news_excerpts_parsed_df[\"Text\"].apply(extract_entities)\n",
    "\n",
    "wikileaks_parsed_df[\"entities\"] = wikileaks_parsed_df[\"Text\"].apply(extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_excerpts_parsed_cleaned_output_path = '/Users/ShanShan/SMU-Datathon-2025/datasets/output/processed_news_excerpts_parsed.csv'\n",
    "\n",
    "# Save to CSV in the specified output directory\n",
    "news_excerpts_parsed_df.to_csv(news_excerpts_parsed_cleaned_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikileaks_parsed_cleaned_output_path = '/Users/ShanShan/SMU-Datathon-2025/datasets/output/processed_wikileaks_parsed.csv'\n",
    "\n",
    "# Save to CSV in the specified output directory\n",
    "wikileaks_parsed_df.to_csv(wikileaks_parsed_cleaned_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
